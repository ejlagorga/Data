<!DOCTYPE HTML>
<!-- 
 **********************************************************

             TUFTS COMP 117 HOMEWORK ASSIGNMENT

        Fill in your name, e-mail and answers to questions
        in the sections provided below.

          All questions are marked up: <li class="q">
          All answers are marked up:   <li class="a">


     PLEASE DO NOT MODIFY ANY THING OTHER THAN THE 
     NAME, E-MAIL, AND ANSWER FIELDS (you may also add
     css declarations in the <head> if you need them,
     but usually you won't.

 **********************************************************
 -->


<html>
<head>
<meta charset="utf-8"> 
<title>Generative Jazz</title>
<link rel="stylesheet" type="text/css" href="paper.css"/>
</head>

<body>

<div class="UpperRightHeader">
<p>Tufts Music 42 (Spring 2018):
<br>
History of Jazz
</p>
</div>

<div class="UpperLeftHeader">
<p><a href="mailto:john.lagorga@tufts.edu">Eli LaGorga</a>
<br>
4/29/2018
</p>
</div>

<div class="Title">
<h1>
Generative Jazz
</h1>
</div>

<div class="Paper">
<p> 
The official specification for Hypertext Transfer Protocol (HTTP) has undergone only one significant functional revision. When RFC 1945 introduced HTTP 1.0 to the world in 1996 it was intended to be a simple and straightforward protocol. It was a 60-page specification which three years later was updated to 176-pages within an entire family of RFCs (1). HTTP 1.1, the protocol for virtually everything on the internet, had arrived. HTTP 1.1 wasn’t so long and didn’t includes innumerable details and subtleties just to confuse users. It was a forward looking effort to create a protocol which would continue to be compatible even as the web grew in unforeseeable directions. Ten years after the introduction of HTTP 1.0, whether the protocol succeeded wasn’t even a question worth asking. The internet had traversed the entire earth and the web had permeated our lives irrevocably. The question that people began to ask was whether HTTP could be improved and if it was even feasible to make another revision. 
</p>
<p> 
HTTP 1.1 had notable list problems from the offset which would only grow as the web developed. The flexibility moving forward meant that HTTP 1.1 had many optional parts which led to a web in which very few implementations accounted for all possible use cases. This resulted in many infrequently used features lacking support from the majority of the web, and implementations which enabled these features getting very little traffic. Eventually, when a seldom used feature became popular, interoperability problems between servers and clients who did not support one feature or another began to proliferate (2). HTTP pipelining is classic example of such a feature. Another issue HTTP 1.1 faced was its inability to take full advantage of the performance advantages Transmission Control Protocol (TCP) offered due to extended pauses in the protocol during which more data could be sent or received. This became more pronounced as the web has matured - a popular web site may now need to resolve over 100 individual resources in order to render the contents of a page (11). This makes HTTP 1.1 very latency sensitive, a problem exacerbated by HTTP pipelining not having been implemented extensively throughout the web. 
</p>
<p> 
As its name suggests HTTP pipelining is a feature that enables a client to make a request while waiting for the response of the previous request. It suffers from head-of-line blocking where due to the sequential processing performance is limited by out-of-order delivery when multiple requests are made (3). One result of high latency and the ever increasing amount of bandwidth was it became faster to send one gigantic image using HTTP 1.1 then it was to send a multitude of smaller images that contain the exact same content in total. This lead programmers to construct piecewise images on the server-side and deconstruct them once they had been received by the client code in a method called sprinting. A similar tactic was applied to transferring javascript code that needed to be run on the client side. Programmers would concatenate all the scripts into one giant file rather than many smaller ones which resulted in large over head when only a small amount of code need to be reloaded. These ad hoc solution are great in some instances but over looks other features of HTTP like caching the most recently used content to minimize redundant requests (2). Another work around for head-of-line blocking on any one TCP connection required creating as many connections between the client and the server as possible. There is a upper bound on the number of connections that can be maintained on one server which was initially strictly enforced by the HTTP 1.1 specification. As a  result services began to use a technique called domain sharding where they would contrive new host names and distribute content across them. Although this regulation has been relaxed the use of this technique proliferated and the number of TCP connections a popular websites made when being displayed ballooned to over 40. Though domain sharding introduced over head of its own. The addition of domains coincided with performance losses as clients needed to perform a DNS lookup for each additional domain and maintain the connection established with said domain for the entirety of the session (4). 
</p>
<p> 
In 2007, with the flaws of HTTP 1.1 in mind, the Internet Engineering Task Force (IETF) started a working group dedicated to the creation of an update for the HTTP specification. Their aim, to create a protocol that was less latency sensitive, fixed HTTP pipelining by resolving head-of-line blocking and eliminated the need to increase the number of TCP connections between clients and servers, all while maintaining the existing interfaces, content, URIs, and schemes. Needless to say this was a bit more than they bargained for and five years later no meaningful progress had been made. So they did what all good computer scientists and system engineers learn to do early on in their formal training, they googled it.
</p>
<p> 
SPDY was an application-layer networking protocol for transporting content over the web developed at google and designed specifically for minimal latency. Google at the time had identified a similar list of problems with the HTTP 1.1 protocol but their issues originated from a commercial perspective. Google wanted to make as many requests as needed per TCP connection, remove the restriction of client-initiated requests, make data compression mandatory, and do it all with better security. In 2012, the group developing SPDY had a public version of the proctol running under their web browser which implementented multiplexed streams that allowed many concurrent HTTP requests to run across a single TCP connection along with a system for request prioritization which alleviated the channel clogging issue that could occur if the bandwidth on a connection was constrained. They also dramatically reduced the number of optional features in HTTP like enforcing mandatory header compression so all requests and responses required fewer bytes to be transmitted. This part of the SPDY protocol essentially solved the issues the IETF had identified five years earlier. By the time the IETF was able to allocate the proper resources to the development of HTTP 2, SPDY had already been deployed over the internet and Google had the numbers to back up their performance claims. The HTTP 2 work began with the SPDY 3 draft which was basically the initial HTTP 2 draft after “a little search and replace. (2)”
</p>
<p> 
This is an obvious hyperbole, recycling the techniques innovated for SPDY into something that maintained HTTP paradigms was difficult because of the strict restraints they enforced, but it does highlight the influential position Google held at the design table and their ability to embed features critical to the growth and monetization of their company into the new protocol. The HTTP 2 protocol couldn’t require changes to existing names which contained “http://” because the sheer quantity of content on the web made it unrealistic to expect them all to change, this restricted their ability to define an entirely new scheme as Google had done. In addition HTTP 1.1 servers and clients were going to persist for decades and the HTTP 2 protocol needed to interpolate with them and therefore it must support proxies which can map HTTP 2 features to HTTP 1.1 clients one-to-one. Fortunately HTTP 1.1 was designed with forward-compatibility in mind and defined a way to do just that. Using the Upgrade: header the server could send the client a response using the new protocol even though the request was received over the old protocol. The round-trip latency penalty was not something that Google would accept because it reduced the response time perceived by their users. Since they initially implemented SPDY over the secure network protocol (SSL) and eventually its successor the Transport Layer Security (TLS), which made the existing network infrastructure more secure even though they introduced a latency penalty, Google developed a new TLS extension which shortcut the negotiation. Using Next Protocol Negotiation (NPN), the server could tell the client which protocols it knows and the client can then use the protocol it prefers. This introduced the notion bidirectional connections which enabled servers to “initiate communication with clients and push data to the client whenever possible (5),” which made SPDY feel a bit stateful. 
</p>
<p> 
These we’re huge points of contention for the members of the IETF working on HTTP 2; mandating a transport layer, reducing the number of optional features, and moving towards a more stateful Hypertext Transfer Protocol may have improved the latency but dramatically reduced the flexibility of the scheme moving forward. “The subject of mandatory TLS caused much hand-wringing and agitated voices in mailing lists and meetings – is it good or is it evil? It was a highly controversial topic” and because there wasn’t consensus HTTP 2 was deployed with TLS as optional. That being said no relevant web-browsers support HTTP 2 without TLS. NPN and bidirectional streams over a secure transport layers did eventually become standardized by the IETF though the Application Layer Protocol Negotiation (ALPN). This solved the latency problem incurred when agreeing upon a protocol by allowing the client to give the server a list of protocols in its order of preference and the server picks the one it wants, instead of the client making the final decision as in NPN, thus also retaining the true statelessness of the protocol. Overall these were fair and practical tradeoffs, the IETF kept HTTP flexible and maintained all existing interfaces, content, URIs, and schemes, which in turn kept the webs Hypertext Transfer Protocols interoperable and retained the exponential value each resource adds theorized in metcalfe's law by avoiding the introduction of partitions to the web. Meanwhile Google integrated features that would allow them to corner the market on web-browsers and build popular low latency dependent web applications as a reward for their innovation.
</p>
<p> 
Currently about a quarter of the web has implemented HTTP 2, doubling it’s percent usage since last year (6). This transition has been aided by Google, who in good faith removed support for SPDY and NPN from their browser in 2016, instead choosing to support HTTP 2. The adoption is in the beginning phase; big companies like Google, Facebook, Wikipedia, Twitter, which need the the improved latency to make their products viable, have transitioned to HTTP 2 but the majority of the web which existed before 2015 hasn’t bothered. This was a predictable pattern and one that was well prepared for during the design of the protocol. There are tons of implementations of HTTP 2, at least one in almost every major programing language (7), so the barrier to entry isn’t a technical one but more a by product of the features HTTP 2 offers not being necessary for the majority of the web. Since the development of HTTP 2 was primarily to fix HTTP pipelining, if the use case did not originally require the use of pipelining which is the situation the majority of the web found itself in, upgrading to HTTP 2 didn’t do a whole lot of good. It also failed to address some of the problems with HTTP 1.1 because it needed to maintain some paradigms like cookies and authorization headers. This made it possible to deploy HTTP 2 without an inconceivable amount of upgrade work that required fundamental parts to be completely replaced or rewritten to remain interoperable with previous protocols.
</p>
<p> 
This is a very natural development pattern for future iterations of Hypertext Text Protocol. Large companies in the information technology space will continue to innovate and develop new internal protocols when the existing ones do not service their needs. Hopefully they will follow the precedent set by Google and do so in an open capacity and eventually include the IETF in the design of the protocol. The IETF will then fit the new technologies into the existing paradigm making sure all the existing protocols on the web continue to interoperable thus retaining the entire value of the web. The companies on the cutting edge will upgrade their servers and web browsers to support the new protocol, everyone else already on the web will not have the resources or need to do so, and new resources added to the web will use a variation of the rule of least power to select the oldest, or simplest to implement, protocol that is sufficient for their use case. 
</p>
<p> 
It’s difficult to determine the direction these developments will take because of just how recently HTTP 2 was developed and it’s limited implementation throughout the web. One interesting protocol being developed at none other then, you guessed it, Google called Quick UDP Internet Connections (QUIC) is an attempted to again improve the latency of HTTP. QUIC on the surface is very similar to SPDY but it is implement over User Datagram Protocol (UDP) instead of TCP. Since TCP is so successful, widely implemented, and deeply ingrained into operating systems, it is practically impossible meaningfully update. Since QUIC is built directly on top of UDP it gives Google to opportunity to reinvent TCP leading some to call it TCP 2 (9). Given the aforementioned context it will be interesting to see if Google achieves the performance increases it is seeking and if the latency boost can withstand the IETF’s ridgid requirements that keep the web unified. But it may be even more interesting to see what happens if these different perspectives can’t be reconciled as they were during the development of HTTP 2. Another interesting protocol WebSocket is a transport layer built on TCP which uses a HTTP friendly Upgrade handshake to create a session like streaming transport which feels alot like SPDY with bidirectional connections and asynchronous message processing but removes almost all transactional semantics so there is very little overhead per message (10). 
</p>
<p> 
Moving forward there is a general consensus that the IETF will continue to look at the semantic backward compatibility of HTTP to see how much they can break without fracturing the web, data-aware header encoding to further reduce the amount of overhead per message, finally make some permanent decision about the security requirements which are still optional in the HTTP 2 specification, and work towards pushing Domain Name System (DNS) and certificate information from servers. But none of these updates are revolutionary enough to force a revision of HTTP. HTTP 3 will most likely be preceded by a SPDY level proof of concept for one or more of the following capabilities. The Exploitation of multicasting where data is sent across the web to several users at the same time. A peer-to-peer distributed HTTP is invented where the workload of the web is partitioned across all participants. A content-centric networking scheme is developed which allows resources to be directly addressable by named data rather than a combinations of DNS and IP addresses. A method for selective encryption minimizes the overhead required for security on the web, or a procedure increases resistance to traffic analysis where information is deduced from the patterns of communication even though the messages themselves are encrypted. A cross-stream compression algorithm is optimized for parallel streaming protocols like HTTP 2. Or a censorship proof protocol is developed which brings the full extent of the webs resources to the entire world regardless of sociopolitical factors (8).
</p>
<p> 
</div>

<div class="Bibliography">
<h1>
Bibliography:
</h1>

<ol>
<li>
<a href="https://tools.ietf.org/html/rfc7230">Hypertext Transfer Protocol (HTTP/1.1): Message Syntax and Routing,</a>
Internet Engineering Task Force
</li>
<li>
<a href="https://http2-explained.haxx.se/content/en/">http2 explained,</a> 
Daniel Haxx
</li>
<li>
<a href="https://hpbn.co/http1x/">
High Performance Browser Networking,</a>
Ilya Grigorik
</li>
<li>
<a href="https://blog.stackpath.com/glossary/domain-sharding/">
What is Domain Sharding?,</a>
Robert Gibb
</li>
<li>
<a href="http://dev.chromium.org/spdy/spdy-whitepaper">
PDY: An experimental protocol for a faster web,</a>
Google
</li>
<li>
<a href="https://w3techs.com/technologies/details/ce-http2/all/all">
Usage of HTTP/2 for websites,</a>
W3Techs
</li>
<li>
<a href="https://github.com/http2/http2-spec/wiki/Implementations">
http2 Implementations,</a>
Daniel Nicoletti
</li>
<li>
<a href="https://github.com/HTTPWorkshop/workshop2016/wiki/Future-of-HTTP#http3">
Future of HTTP,</a>
Internet Engineering Task Force
</li>
<li>
<a href="https://twitter.com/ietfmemes/status/757866970939330560 ">
Jana doesn’t like it.</a>
IETF Memes
</li>
<li>
<a href=" https://tools.ietf.org/html/rfc6455">
The WebSocket Protocol,</a>
kInternet Engineering Task Force
</li>
<li>
<a href="https://httparchive.org/reports/state-of-the-web">
Report: State of the Web,</a>
http archive
</li>
</ol>
</div>

</div>
</body>
</html>
